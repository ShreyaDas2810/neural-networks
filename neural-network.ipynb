{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9044572,"sourceType":"datasetVersion","datasetId":5452872}],"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\nclass Linear:\n    \"\"\"\n    Implements a fully connected (dense) layer.\n    \"\"\"\n    def __init__(self, input_dim, output_dim):\n        \"\"\"\n        Initializes the layer with weights and biases.\n        \"\"\"\n        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n        self.biases = np.zeros((1, output_dim))\n        self.input = None\n\n    def forward(self, input_data):\n        \"\"\"\n        Performs the forward pass.\n        \"\"\"\n        self.input = input_data\n        return np.dot(input_data, self.weights) + self.biases\n\n    def backward(self, grad_output, learning_rate):\n        \"\"\"\n        Performs the backward pass and updates the weights and biases.\n        \"\"\"\n        grad_input = np.dot(grad_output, self.weights.T)\n        grad_weights = np.dot(self.input.T, grad_output)\n        grad_biases = np.sum(grad_output, axis=0, keepdims=True)\n        \n        self.weights -= learning_rate * grad_weights\n        self.biases -= learning_rate * grad_biases\n        \n        return grad_input\n\n\nclass ReLU:\n    \"\"\"\n    Implements the ReLU activation function.\n    \"\"\"\n    def forward(self, input_data):\n        \"\"\"\n        Applies the ReLU activation function.\n        \"\"\"\n        self.input = input_data\n        return np.maximum(0, input_data)\n    \n    def backward(self, grad_output):\n        \"\"\"\n        Computes the gradient of the ReLU function.\n        \"\"\"\n        return grad_output * (self.input > 0)\n\n\nclass Sigmoid:\n    \"\"\"\n    Implements the Sigmoid activation function.\n    \"\"\"\n    def forward(self, input_data):\n        \"\"\"\n        Applies the Sigmoid activation function.\n        \"\"\"\n        self.output = 1 / (1 + np.exp(-input_data))\n        return self.output\n    \n    def backward(self, grad_output):\n        \"\"\"\n        Computes the gradient of the Sigmoid function.\n        \"\"\"\n        return grad_output * self.output * (1 - self.output)\n\n\nclass Tanh:\n    \"\"\"\n    Implements the Tanh activation function.\n    \"\"\"\n    def forward(self, input_data):\n        \"\"\"\n        Applies the Tanh activation function.\n        \"\"\"\n        self.output = np.tanh(input_data)\n        return self.output\n    \n    def backward(self, grad_output):\n        \"\"\"\n        Computes the gradient of the Tanh function.\n        \"\"\"\n        return grad_output * (1 - self.output ** 2)\n\n\nclass Softmax:\n    \"\"\"\n    Implements the Softmax activation function.\n    \"\"\"\n    def forward(self, input_data):\n        \"\"\"\n        Applies the Softmax activation function.\n        \"\"\"\n        exp_scores = np.exp(input_data - np.max(input_data, axis=1, keepdims=True))\n        self.output = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n        return self.output\n    \n    def backward(self, grad_output):\n        \"\"\"\n        Computes the gradient of the Softmax function.\n        \"\"\"\n        # The derivative is complex; typically handled in combination with cross-entropy loss\n        raise NotImplementedError(\"Backward pass for Softmax is not implemented, typically handled with combined loss\")\n\n\nclass CrossEntropyLoss:\n    \"\"\"\n    Implements the Cross-Entropy loss function.\n    \"\"\"\n    def forward(self, predictions, targets):\n        \"\"\"\n        Computes the forward pass of the Cross-Entropy loss.\n        \"\"\"\n        self.predictions = predictions\n        self.targets = targets\n        m = targets.shape[0]\n        log_likelihood = -np.log(predictions[range(m), targets])\n        loss = np.sum(log_likelihood) / m\n        return loss\n\n    def backward(self):\n        \"\"\"\n        Computes the gradient of the Cross-Entropy loss.\n        \"\"\"\n        m = self.targets.shape[0]\n        grad = self.predictions\n        grad[range(m), self.targets] -= 1\n        grad /= m\n        return grad\n\n\nclass MSELoss:\n    \"\"\"\n    Implements the Mean Squared Error loss function.\n    \"\"\"\n    def forward(self, predictions, targets):\n        \"\"\"\n        Computes the forward pass of the MSE loss.\n        \"\"\"\n        self.predictions = predictions\n        self.targets = targets\n        loss = np.mean((predictions - targets) ** 2)\n        return loss\n\n    def backward(self):\n        \"\"\"\n        Computes the gradient of the MSE loss.\n        \"\"\"\n        return 2 * (self.predictions - self.targets) / self.targets.shape[0]\n\n\nclass SGD:\n    \"\"\"\n    Implements the Stochastic Gradient Descent optimizer.\n    \"\"\"\n    def __init__(self, learning_rate):\n        \"\"\"\n        Initializes the SGD optimizer with a given learning rate.\n        \"\"\"\n        self.learning_rate = learning_rate\n    \n    def step(self, params):\n        \"\"\"\n        Updates the parameters using the computed gradients.\n        \"\"\"\n        for param in params:\n            params[param] -= self.learning_rate * params[param + '_grad']\n\n\nclass Model:\n    \"\"\"\n    Implements a neural network model.\n    \"\"\"\n    def __init__(self):\n        self.layers = []\n        self.loss_function = None\n        self.optimizer = None\n\n    def add_layer(self, layer):\n        \"\"\"\n        Adds a layer to the model.\n        \"\"\"\n        self.layers.append(layer)\n    \n    def compile(self, loss_function, optimizer):\n        \"\"\"\n        Compiles the model with a loss function and optimizer.\n        \"\"\"\n        self.loss_function = loss_function\n        self.optimizer = optimizer\n    \n    def forward(self, input_data):\n        \"\"\"\n        Performs the forward pass through all layers.\n        \"\"\"\n        for layer in self.layers:\n            input_data = layer.forward(input_data)\n        return input_data\n\n    def backward(self, grad_output):\n        \"\"\"\n        Performs the backward pass through all layers.\n        \"\"\"\n        for layer in reversed(self.layers):\n            grad_output = layer.backward(grad_output, self.optimizer.learning_rate)\n    \n    def train(self, X_train, y_train, epochs, batch_size):\n        \"\"\"\n        Trains the model on the provided data.\n        \"\"\"\n        num_samples = X_train.shape[0]\n        for epoch in range(epochs):\n            for i in range(0, num_samples, batch_size):\n                X_batch = X_train[i:i + batch_size]\n                y_batch = y_train[i:i + batch_size]\n\n                # Forward pass\n                predictions = self.forward(X_batch)\n                loss = self.loss_function.forward(predictions, y_batch)\n                \n                # Backward pass\n                grad = self.loss_function.backward()\n                self.backward(grad)\n\n                # Update parameters\n                for layer in self.layers:\n                    if isinstance(layer, Linear):\n                        self.optimizer.step({'weights': layer.weights, 'biases': layer.biases})\n                \n                print(f'Epoch {epoch + 1}/{epochs}, Loss: {loss}')\n    \n    def predict(self, X):\n        \"\"\"\n        Predicts the outputs for given inputs.\n        \"\"\"\n        return self.forward(X)\n    \n    def evaluate(self, X_test, y_test):\n        \"\"\"\n        Evaluates the model on the test data.\n        \"\"\"\n        predictions = self.predict(X_test)\n        loss = self.loss_function.forward(predictions, y_test)\n        accuracy = np.mean(np.argmax(predictions, axis=1) == y_test)\n        return loss, accuracy\n\n    def save(self, filename):\n        \"\"\"\n        Saves the model's weights to a file.\n        \"\"\"\n        np.savez(filename, **{f'{layer.__class__.__name__}_weights': layer.weights for layer in self.layers if isinstance(layer, Linear)},\n                           **{f'{layer.__class__.__name__}_biases': layer.biases for layer in self.layers if isinstance(layer, Linear)})\n    \n    def load(self, filename):\n        \"\"\"\n        Loads the model's weights from a file.\n        \"\"\"\n        data = np.load(filename)\n        for layer in self.layers:\n            if isinstance(layer, Linear):\n                layer.weights = data[f'{layer.__class__.__name__}_weights']\n                layer.biases = data[f'{layer.__class__.__name__}_biases']\nfrom neural_network_framework import Model, CrossEntropyLoss, SGD, Linear, ReLU, Softmax\nimport numpy as np\n\n# Example usage\n\n# Define a simple neural network using the framework\nmodel = Model()\nmodel.add_layer(Linear(784, 128))\nmodel.add_layer(ReLU())\nmodel.add_layer(Linear(128, 10))\nmodel.add_layer(Softmax())\n\n# Compile the model with loss and optimizer\nloss = CrossEntropyLoss()\noptimizer = SGD(learning_rate=0.01)\nmodel.compile(loss, optimizer)\n\n# Assume x_train, y_train, x_test, y_test are preprocessed and available\n# For example purposes, we use random data here\nx_train = np.random.randn(1000, 784)\ny_train = np.random.randint(0, 10, size=(1000,))\nx_test = np.random.randn(200, 784)\ny_test = np.random.randint(0, 10, size=(200,))\n\n# Train the model\nmodel.train(x_train, y_train, epochs=20, batch_size=64)\n\n# Evaluate the model\ntest_loss, test_accuracy = model.evaluate(x_test, y_test)\nprint(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n","metadata":{},"execution_count":null,"outputs":[]}]}